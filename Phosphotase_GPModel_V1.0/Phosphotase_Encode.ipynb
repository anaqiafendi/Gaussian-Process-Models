{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitgpmodelcondadf662a052f7548da8f1cfb5439246441",
   "display_name": "Python 3.6.10 64-bit ('GP_Model': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\Users\\kikir\\anaconda3\\envs\\GP_Model\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
    }
   ],
   "source": [
    "import encoding_tools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# ML imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "from scipy import optimize, linalg\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom imports\n",
    "import encoding_tools as encoding\n",
    "import chimera_tools as chimera\n",
    "import GP_tools as GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Folder = Path(r\"Phosphotase_Encode.ipynb\").parent.absolute() / Path(\"Processed Data\")\n",
    "\n",
    "dicts = ['EFI_ID_List', 'metabolite_dict', 'Protein_seq_dict']\n",
    "\n",
    "with open(Processed_Folder / Path('EFI_ID_List.p'), 'rb') as EFI_ID:\n",
    "    EFI_ID_List = pickle.load(EFI_ID)\n",
    "\n",
    "with open(Processed_Folder / Path('metabolite_dict.p'), 'rb') as metabolite:\n",
    "    metabolite_dict = pickle.load(metabolite)\n",
    "\n",
    "with open(Processed_Folder / Path('Protein_seq_dict.p'), 'rb') as Protein_seq:\n",
    "    Protein_seq_dict = pickle.load(Protein_seq)\n",
    "\n",
    "with open(Processed_Folder / Path('Protein_aligned_dict.p'), 'rb') as Protein_aln:\n",
    "    Protein_aligned_dict = pickle.load(Protein_aln)\n",
    "\n",
    "activations = pd.read_csv(Processed_Folder / Path('activations.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to pad protein sequences to the max length of the longest one\n",
    "max_len = len(max(Protein_aligned_dict.values(), key=len))\n",
    "fillchar = '-' # This is whats used in the GP-UCB paper\n",
    "Padded_dict = {}\n",
    "OH_dict = {}\n",
    "for ID in EFI_ID_List:\n",
    "    Padded_dict[ID] = Protein_seq_dict[ID].upper().ljust(max_len, fillchar)\n",
    "    OH_dict[ID] = encoding_tools.one_hot_seq(seq_input=Padded_dict[ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n768\n169\n"
    }
   ],
   "source": [
    "ID = np.random.randint(low=0,high=218)\n",
    "len_comp = len(Padded_dict[EFI_ID_List[ID]])\n",
    "print(len(Padded_dict[EFI_ID_List[0]]) == len(Padded_dict[EFI_ID_List[ID]]))\n",
    "print(len_comp)\n",
    "print(ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_train(X, y):\n",
    "    # test the optimization of the hyp-prams\n",
    "    initial_guess = [0.9,0.9]\n",
    "\n",
    "    # take the log of the initial guess for optimiziation \n",
    "    initial_guess_log = np.log(initial_guess)\n",
    "\n",
    "    # optimize to fit model\n",
    "    result = scipy.optimize.minimize(GP.neg_log_marg_likelihood, initial_guess_log, args=(X,y), method='L-BFGS-B')\n",
    "    \n",
    "    print('Full GP regression model')\n",
    "    print('Hyperparameters: ' + str(np.exp(result.x[0])) + ' ' + str(np.exp(result.x[1])))\n",
    "\n",
    "    # next set of hyper prams \n",
    "    final_prams = [np.exp(result.x[0]), np.exp(result.x[1])]\n",
    "    \n",
    "    return final_prams\n",
    "\n",
    "def ML_predict(X, y, X_true_test, y_true_test, log_data, final_prams, property_, num_iter=1, Substrate_ID=0, metabolite_dict=metabolite_dict):\n",
    "    substrate = list(metabolite_dict.values())[Substrate_ID]\n",
    "    \n",
    "    if not os.path.exists('outputs/loop_figs_aln/' + str(substrate) + '/'):\n",
    "        os.makedirs('outputs/loop_figs_aln/' + str(substrate) + '/')\n",
    "\n",
    "    path_outputs = 'outputs/loop_figs_aln/' + str(substrate) + '/'\n",
    "\n",
    "    # next use trained GP model to predict full test set\n",
    "    mu_true_test, var_true_test = GP.predict_GP(X, y, X_true_test, final_prams)\n",
    "\n",
    "    # convert the true test predications and y back to unnormalized data\n",
    "    y_test_real = np.exp(y_true_test*np.std(log_data)  + np.mean(log_data))\n",
    "    mu_test_real = np.exp(mu_true_test*np.std(log_data)  + np.mean(log_data))\n",
    "\n",
    "    if property_ != 'kinetics_off':\n",
    "        \n",
    "        par = np.polyfit(y_test_real, mu_test_real, 1, full=True)\n",
    "        slope=par[0][0]\n",
    "        intercept=par[0][1]\n",
    "        \n",
    "        # coefficient of determination, plot text\n",
    "        variance = np.var(mu_test_real)\n",
    "        residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(y_test_real, mu_test_real)])\n",
    "        Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "        print('GP regression model test set')\n",
    "        print('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        \n",
    "        # plot and measure correlation\n",
    "        plt.figure('True test', figsize=(1.5, 1.5))\n",
    "        plt.plot(y_test_real, mu_test_real, 'o', ms=3, color='k')\n",
    "        \n",
    "        max_x = np.max(y_test_real)\n",
    "        plt.plot([0, max_x], [intercept, slope*max_x+intercept], '-', color='k')\n",
    "        plt.suptitle('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        plt.savefig(path_outputs + str(property_)+'_matern_kernel_'+str(num_iter)+'.png', bbox_inches='tight', transparent=False, dpi=300)\n",
    "        plt.clf()\n",
    "        # plt.show()\n",
    "\n",
    "    elif property_ == 'kinetics_off':\n",
    "        \n",
    "        par = np.polyfit(np.log10(y_test_real), np.log10(mu_test_real), 1, full=True)\n",
    "        slope=par[0][0]\n",
    "        intercept=par[0][1]\n",
    "        \n",
    "        # coefficient of determination, plot text\n",
    "        variance = np.var(np.log10(mu_test_real))\n",
    "        residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(np.log10(y_test_real), np.log10(mu_test_real))])\n",
    "        Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "        print('GP regression model test set')\n",
    "        print('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        \n",
    "        # plot and measure correlation\n",
    "        plt.figure('True test', figsize=(1.5, 1.5))\n",
    "        plt.plot(np.log10(y_test_real), np.log10(mu_test_real), 'o',  ms=3, color='k')\n",
    "        \n",
    "        max_x = np.max(y_test_real)\n",
    "        min_x = np.min(y_test_real)\n",
    "        \n",
    "        plt.plot([np.log10(min_x), np.log10(max_x)], [np.log10(slope*min_x+intercept), np.log10(slope*max_x+intercept)], '-', color='k')\n",
    "        \n",
    "        plt.savefig(path_outputs + str(property_)+'_matern_kernel_'+str(num_iter)+'.png', bbox_inches='tight', transparent=True)\n",
    "        # plt.show()\n",
    "\n",
    "    # df_select_test_not_defined\n",
    "    df_select_test = pd.DataFrame(columns=['y','mu','y_real','mu_real'])\n",
    "\n",
    "    # export csv with predicted values\n",
    "    df_select_test['y'] = y_true_test\n",
    "    df_select_test['mu'] = mu_true_test\n",
    "    df_select_test['y_real'] = y_test_real\n",
    "    df_select_test['mu_real'] = mu_test_real\n",
    "\n",
    "    df_select_test.to_csv(path_outputs+ 'matern_kernel_'+str(num_iter)+'_'+str(property_)+'.csv')\n",
    "    return np.sqrt(Rsqr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, log_data, property_):\n",
    "    path_outputs = 'outputs/loop_figs/'\n",
    "\n",
    "    kf = KFold(n_splits=20) # Define the split\n",
    "    kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "    mu_s = []\n",
    "    var_s = []\n",
    "    y_s = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "\n",
    "        log_data_train, log_data_test = log_data[train_index], log_data[test_index]\n",
    "\n",
    "        y_train = (log_data_train - np.mean(log_data_train))/np.std(log_data_train)\n",
    "        y_test = (log_data_test - np.mean(log_data_train))/np.std(log_data_train)\n",
    "\n",
    "        initial_guess = [0.1,10]\n",
    "\n",
    "        # take the log of the initial guess for optimiziation \n",
    "        initial_guess_log = np.log(initial_guess)\n",
    "\n",
    "        # optimize to fit model\n",
    "        result = scipy.optimize.minimize(GP.neg_log_marg_likelihood, initial_guess_log, args=(X_train,y_train), method='L-BFGS-B')#,\n",
    "\n",
    "        # next set of hyper prams \n",
    "        prams_me = [np.exp(result.x[0])**2, np.exp(result.x[1])]\n",
    "\n",
    "        # next used trained GP model to predict on test data\n",
    "        mu, var = GP.predict_GP(X_train, y_train, X_test, prams_me)\n",
    "        \n",
    "        # un normalize\n",
    "        y_test_real = np.exp(y_test*np.std(log_data_train)  + np.mean(log_data_train))\n",
    "        mu_real = np.exp(mu*np.std(log_data_train)  + np.mean(log_data_train))\n",
    "        \n",
    "        mu_s.append(mu)\n",
    "        var_s.append(var)\n",
    "        y_s.append(y_test)\n",
    "\n",
    "    # reformat all\n",
    "    y_s_all = [j for i in y_s for j in i]\n",
    "    mu_s_all = [j for i in mu_s for j in i]\n",
    "\n",
    "    # plot results\n",
    "    plt.figure('My GP test set evaluation', figsize=(1.5, 1.5))\n",
    "    plt.plot(y_s_all, mu_s_all, 'o', ms=3, color='k')\n",
    "\n",
    "\n",
    "    # calculate correlation \n",
    "    measured = y_s_all\n",
    "    predicted = mu_s_all\n",
    "\n",
    "    par = np.polyfit(measured, predicted, 1, full=True)\n",
    "    slope=par[0][0]\n",
    "    intercept=par[0][1]\n",
    "\n",
    "    # calc correlation \n",
    "    variance = np.var(predicted)\n",
    "    residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(measured, predicted)])\n",
    "    Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "    \n",
    "    print('20-fold corss validation of GP regression model')\n",
    "    print('R = %0.2f'% np.sqrt(Rsqr))\n",
    "\n",
    "    max_x = np.max(y_s_all)\n",
    "    min_x = np.min(y_s_all)\n",
    "    \n",
    "    plt.plot([min_x, max_x], [slope*min_x+intercept, slope*max_x+intercept], '-', color='k')\n",
    "    plt.savefig(path_outputs + str(property_)+'_matern_kernel_CV_fig1.pdf', bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "    return measured, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform CV train\n",
    "# cross_validation(X=X, log_data=log_data, property_='activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_length = [0]*218\n",
    "\n",
    "for i in range(0,len(Protein_seq_dict.values())):\n",
    "    p_length[i] = len(Protein_seq_dict[EFI_ID_List[i]])\n",
    "\n",
    "plt.hist(p_length,bins=600)\n",
    "plt.xlabel('Protein Sequence Length')\n",
    "plt.ylabel('Number of protein sequences')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format(X, y):\n",
    "    # test data only includes gen 10\n",
    "    # df_test_data = df[df.gen == 10]\n",
    "\n",
    "    # training data excludes test data (gen 10)\n",
    "    # df_data = df[df.gen != 10]\n",
    "\n",
    "    # Clean 0 y data which skews results when having to convert infs\n",
    "    X = X[y != 0]\n",
    "    y = y[y != 0]\n",
    "\n",
    "    # Use random split of data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # normalize training data\n",
    "    log_data = np.log(y_train)\n",
    "\n",
    "    '''\n",
    "    # Convert -infs to large negative values\n",
    "    for i in range(0,len(log_data)):\n",
    "        if str(log_data[i]) == '-inf':\n",
    "            log_data[i] = -1000\n",
    "    '''\n",
    "\n",
    "    y_train = (log_data - np.mean(log_data))/np.std(log_data)\n",
    "    # seq = df_select.seq.values\n",
    "\n",
    "    # normalize test data\n",
    "    log_data_test = np.log(y_test)\n",
    "    y_test = (log_data_test - np.mean(log_data))/np.std(log_data)\n",
    "    # seq_test = df_select_test.seq.values\n",
    "\n",
    "    return log_data, X_train, X_test, y_train, y_test \n",
    "    \n",
    "    # seq_test, df_select, df_select_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(Protein_seq_dict,EFI_ID_List,activations,Substrate_ID=0,trim_long=False):\n",
    "    \n",
    "    # Trim length of protein sequences first\n",
    "    if trim_long == True:\n",
    "        Trimmed_dict = {}\n",
    "        New_ID_List = []\n",
    "        for ID in EFI_ID_List:\n",
    "            if len(Protein_seq_dict[ID]) <= 300:\n",
    "                Trimmed_dict[ID] = Protein_seq_dict[ID]\n",
    "                New_ID_List.append(ID)\n",
    "        Protein_seq_dict = Trimmed_dict\n",
    "        EFI_ID_List = New_ID_List\n",
    "    \n",
    "    # Need to pad protein sequences to the max length of the longest one\n",
    "    max_len = len(max(Protein_seq_dict.values(), key=len))\n",
    "    fillchar = '-' # This is whats used in the GP-UCB paper\n",
    "    Padded_dict = {}\n",
    "    OH_dict = {}\n",
    "    for ID in EFI_ID_List:\n",
    "        Padded_dict[ID] = Protein_seq_dict[ID].upper().ljust(max_len, fillchar)\n",
    "        OH_dict[ID] = encoding_tools.one_hot_seq(seq_input=Padded_dict[ID])\n",
    "\n",
    "    # Preparing input training data X to feed into ML Model\n",
    "    input_len = len(OH_dict[EFI_ID_List[0]])*21\n",
    "    num_inputs = len(OH_dict.keys())\n",
    "\n",
    "    X = np.zeros((num_inputs,input_len))\n",
    "    for i in range(0,len(EFI_ID_List)):\n",
    "        ID = EFI_ID_List[i]\n",
    "        X_seq = OH_dict[ID]\n",
    "        X_seq = np.reshape(X_seq,(1,X_seq.shape[0]*21))\n",
    "        X[i,:] = X_seq\n",
    "\n",
    "    # Preapre output training data y to feed into ML Model\n",
    "    dummy = [str(ID) for ID in EFI_ID_List]\n",
    "    y = activations[dummy].values[Substrate_ID,:]\n",
    "\n",
    "    '''\n",
    "    # Now we need to normalize the data\n",
    "    ss1 = preprocessing.StandardScaler()\n",
    "    y = y.reshape(-1,1) # Single feature dataset\n",
    "    y = ss1.fit_transform(y) # fit the SS1 and standardize x_train\n",
    "    y = ss1.transform(y) # transform x_test\n",
    "    '''\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_train_test(Protein_seq_dict, EFI_ID_List, activations, Substrate_ID, num_iter, metabolite_dict=metabolite_dict):\n",
    "    # Need to prep inputs X and Y\n",
    "    X, y = prep_inputs(Protein_seq_dict,EFI_ID_List,activations,Substrate_ID=Substrate_ID,trim_long=False)\n",
    "    # Now we test on aligned inputs, new alignment is ~700 long\n",
    "    # But the seq > 300 in length already trimmed from dataset\n",
    "\n",
    "    # Format data for train and test splits\n",
    "    log_data, X_train, X_test, y_train, y_test = data_format(X, y)\n",
    "\n",
    "    # Train ML Model on training set\n",
    "    final_prams = ML_train(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    R = ML_predict(X=X_train, y=y_train, X_true_test=X_test, y_true_test=y_test, log_data=log_data, final_prams=final_prams, property_='activity', num_iter=num_iter, Substrate_ID=Substrate_ID, metabolite_dict=metabolite_dict)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Full GP regression model\nHyperparameters: 9.63919909586e-07 6.41790298093\nGP regression model test set\nR = 0.574\nFull GP regression model\nHyperparameters: 4.58208222101e-08 7.24015793532\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 7.19966967084e-06 5.88766044451\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 9.32760654085e-07 6.59171050339\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 1.32630146746e-06 6.36528131418\nGP regression model test set\nR = 0.332\nFull GP regression model\nHyperparameters: 1.72188353977e-08 9.33278677366\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 7.7901119376e-07 6.53420392937\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 1.8236095264e-06 7.42761991972\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 9.04739379889e-07 8.46609799041\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 7.10959583783e-07 5.53734118755\nGP regression model test set\nR = 0.412\nFull GP regression model\nHyperparameters: 1.15228836763e-06 6.7152411977\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 8.11667952417e-07 7.95962901817\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 2.92938501652e-06 6.65753922895\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 1.96183247547e-06 4.5672041029\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 1.96212442224e-06 7.05372141648\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 0.000503943474017 0.900444356466\nGP regression model test set\nR = 0.387\nFull GP regression model\nHyperparameters: 2.34987694265e-06 7.21811153432\nGP regression model test set\nR = 0.400\nFull GP regression model\nHyperparameters: 9.06532793581e-07 8.55326243694\nGP regression model test set\nR = 0.265\nFull GP regression model\nHyperparameters: 4.87349953788e-06 5.72865396444\nGP regression model test set\nR = 0.100\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 4.44644295023e-05 2.5690672435\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 1.24952838938e-06 5.21056682797\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 0.00152082959193 4.87005369099\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 4.78503353611e-06 7.0353242513\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 6.36262691523e-06 4.24992607337\nGP regression model test set\nR = 0.374\nFull GP regression model\nHyperparameters: 0.000236697186309 5.00048109385\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 4.91449253204e-05 2.93579042953\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 0.0147067723577 6.48532794281\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 1.34819359807e-06 6.1009491473\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 1.15295265146e-06 4.05254754122\nGP regression model test set\nR = 0.224\nFull GP regression model\nHyperparameters: 6.70096934745e-06 4.82336738485\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 0.000809087857021 4.67999666079\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 0.000969457313837 5.39110678803\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 2.87044458575e-06 5.68764204165\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 0.000996014706634 5.18276745401\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 7.64871211056e-05 3.35903443326\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 1.61745858016e-06 5.50702967505\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 3.22146187464e-06 5.20126436125\nGP regression model test set\nR = 0.224\nFull GP regression model\nHyperparameters: 2.8510749164e-06 3.77534820607\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 1.55728837574e-06 5.05216952426\nGP regression model test set\nR = 0.100\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.000116294253487 11.0293186624\nGP regression model test set\nR = 0.458\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 2.26227428995e-07 10.9909039924\nGP regression model test set\nR = 0.387\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 5.89052330804e-07 10.04676719\nGP regression model test set\nR = 0.387\nFull GP regression model\nHyperparameters: 4.10152750496e-05 9.76400258472\nGP regression model test set\nR = 0.332\nFull GP regression model\nHyperparameters: 5.92465263988e-07 10.3001185182\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 6.56362210871e-07 10.3370272175\nGP regression model test set\nR = 0.300\nFull GP regression model\nHyperparameters: 6.59604208519e-08 10.1256956635\nGP regression model test set\nR = 0.583\nFull GP regression model\nHyperparameters: 2.49496847426e-07 10.1193959288\nGP regression model test set\nR = 0.412\nFull GP regression model\nHyperparameters: 0.0373555896285 11.899408042\nGP regression model test set\nR = 0.447\nFull GP regression model\nHyperparameters: 1.12809441311e-07 11.3522992092\nGP regression model test set\nR = 0.265\nFull GP regression model\nHyperparameters: 3.39806029539e-06 8.93194377416\nGP regression model test set\nR = 0.447\nFull GP regression model\nHyperparameters: 8.16413582618e-07 10.2095322574\nGP regression model test set\nR = 0.436\nFull GP regression model\nHyperparameters: 1.22057270681e-06 10.9762068184\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 2.59058821717e-06 10.5500999158\nGP regression model test set\nR = 0.346\nFull GP regression model\nHyperparameters: 0.0818548341738 8.93204918934\nGP regression model test set\nR = 0.245\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0699134147218 9.29772116074\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 1.89329444137e-06 7.28898128702\nGP regression model test set\nR = 0.548\nFull GP regression model\nHyperparameters: 0.10178365673 10.1873519934\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 0.0612303008976 10.5504249218\nGP regression model test set\nR = 0.316\nFull GP regression model\nHyperparameters: 0.0952955064418 9.64276907663\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 0.039487261738 7.93316760785\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 0.00864225153613 8.98854626038\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 0.00796452544234 5.70261981071\nGP regression model test set\nR = 0.447\nFull GP regression model\nHyperparameters: 0.125601599685 10.6704486845\nGP regression model test set\nR = 0.224\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0277083368207 6.79984465914\nGP regression model test set\nR = 0.412\nFull GP regression model\nHyperparameters: 2.18792854814e-06 6.57258912386\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 0.00928799099492 6.23848674916\nGP regression model test set\nR = 0.265\nFull GP regression model\nHyperparameters: 0.00643532399876 5.61560918343\nGP regression model test set\nR = 0.361\nFull GP regression model\nHyperparameters: 0.12505990602 10.3447422761\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 0.0552970012959 8.14371535055\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 0.0255522578248 6.8471505492\nGP regression model test set\nR = 0.224\nFull GP regression model\nHyperparameters: 0.00902559241558 10.1421406982\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 0.0699183211084 9.08737011469\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 0.0537938996271 8.59242691911\nGP regression model test set\nR = 0.200\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.000165967114672 0.650298143956\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 0.000325885167946 0.567384388949\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 0.0131837812996 6.94350287903\nGP regression model test set\nR = 0.265\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0492610690499 9.38941446997\nGP regression model test set\nR = 0.316\nFull GP regression model\nHyperparameters: 0.00608991552014 6.73297081715\nGP regression model test set\nR = 0.346\nFull GP regression model\nHyperparameters: 0.579276586177 26.3040510704\nGP regression model test set\nR = 0.361\nFull GP regression model\nHyperparameters: 0.000195118306292 7.4481432731\nGP regression model test set\nR = 0.374\nFull GP regression model\nHyperparameters: 0.108521812933 11.1869157281\nGP regression model test set\nR = 0.200\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0275975565781 8.82501665067\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 0.000273068003255 0.584091682109\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 0.000434416805245 7.05453948281\nGP regression model test set\nR = 0.332\nFull GP regression model\nHyperparameters: 2.7130034253e-05 6.00987674259\nGP regression model test set\nR = 0.346\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 3.34132890276e-05 7.00734251483\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 3.89285626913e-06 6.56280538355\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 0.0143966516824 8.62239631723\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 6.62759250765e-06 6.50382499387\nGP regression model test set\nR = 0.224\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0265144631798 9.98926560317\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 8.97579814308e-07 5.67997732616\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 6.39295876572e-06 7.76553346805\nGP regression model test set\nR = 0.141\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 1.74983238428e-05 5.39348574709\nGP regression model test set\nR = 0.300\nFull GP regression model\nHyperparameters: 5.59714899371e-07 7.08822173222\nGP regression model test set\nR = 0.265\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.000327792502138 0.783695643051\nGP regression model test set\nR = 0.245\nFull GP regression model\nHyperparameters: 0.00302057212551 7.52393356528\nGP regression model test set\nR = 0.200\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.0272239393864 8.83163186906\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 9.85057353004e-05 8.67937479306\nGP regression model test set\nR = 0.000\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 2.69589137803e-07 4.82595592505\nGP regression model test set\nR = 0.283\nFull GP regression model\nHyperparameters: 4.36630039152e-06 5.84629864559\nGP regression model test set\nR = 0.000\nFull GP regression model\nHyperparameters: 9.0289278152e-07 5.34927676246\nGP regression model test set\nR = 0.447\nFull GP regression model\nHyperparameters: 2.25381310537e-07 5.07594112451\nGP regression model test set\nR = 0.141\nFull GP regression model\nHyperparameters: 1.48384318437e-05 5.59082086189\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 0.000452955325688 0.941995636019\nGP regression model test set\nR = 0.200\nLinAlgError, negative value in eigenvector\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 2.25565968071e-06 5.47553216605\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 8.1438500145e-07 5.8643526004\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 0.000452128153159 0.923441039121\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 8.09837699298e-05 5.54822429085\nGP regression model test set\nR = 0.000\nLinAlgError, negative value in eigenvector\nFull GP regression model\nHyperparameters: 0.000485159403438 0.900435462881\nGP regression model test set\nR = 0.200\nFull GP regression model\nHyperparameters: 0.00046000400784 0.9\nGP regression model test set\nR = 0.100\nFull GP regression model\nHyperparameters: 9.20262846355e-07 4.7785633643\nGP regression model test set\nR = 0.447\nFull GP regression model\nHyperparameters: 1.45165234412e-05 5.19914119572\nGP regression model test set\nR = 0.173\nFull GP regression model\nHyperparameters: 2.84002009457e-06 6.8317862435\nGP regression model test set\nR = 0.200\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1a7fb1027750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauto_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mProtein_seq_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mProtein_aligned_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEFI_ID_List\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEFI_ID_List\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubstrate_ID\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmetabolite_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetabolite_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mR_Results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-fb2ba5888877>\u001b[0m in \u001b[0;36mauto_train_test\u001b[1;34m(Protein_seq_dict, EFI_ID_List, activations, Substrate_ID, num_iter, metabolite_dict)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Format data for train and test splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mlog_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Train ML Model on training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2727390cec8e>\u001b[0m in \u001b[0;36mdata_format\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Use random split of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# normalize training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GP_Model\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2070\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2072\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GP_Model\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2070\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2072\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\GP_Model\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "metabolite_list = list(metabolite_dict.values())\n",
    "\n",
    "r_low = 0 # Index of starting metabolite in metabolite list (substrates)\n",
    "r_high = 60 # 168 total substrates, do batches of 60,60,48 -> 0-60, 60-120, 120-168\n",
    "batch_size = r_high - r_low\n",
    "num_iters = 20 # 20 trials of split, train, test\n",
    "\n",
    "R_Results = pd.DataFrame(columns=metabolite_list[r_low:r_high], index=range(0,num_iters))\n",
    "EFI_ID_List = list(Protein_aligned_dict.keys())\n",
    "\n",
    "for j in range(r_low,r_high):\n",
    "    for i in range(0,num_iters):\n",
    "        try:\n",
    "            R = auto_train_test(Protein_seq_dict=Protein_aligned_dict, EFI_ID_List=EFI_ID_List, activations=activations, Substrate_ID=j, num_iter=i,  metabolite_dict=metabolite_dict)\n",
    "            R_Results.iloc[i,int(j % batch_size)] = R\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            R_Results.iloc[i,int(j % batch_size)] = float(\"NaN\")\n",
    "            print('LinAlgError, negative value in eigenvector')\n",
    "\n",
    "path_outputs = 'outputs/loop_figs_aln/'\n",
    "R_Results.to_csv(path_outputs+ 'R_Values_Metabolites_'+str(r_low)+'-'+str(r_high)+'_'+str(num_iters)+'times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Results.to_csv(path_outputs+ 'R_Values_Metabolites_'+str(r_low)+'-'+str(r_high)+'_'+str(num_iters)+'times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R_Results = pd.DataFrame(columns=list(metabolite_dict.values()), index=list(range(0,20)))\n",
    "for i in range(0,20):\n",
    "    print(R_Results.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 0 - 80 metabolites\n",
    "# Next 81 - 168\n",
    "len(metabolite_dict)"
   ]
  }
 ]
}