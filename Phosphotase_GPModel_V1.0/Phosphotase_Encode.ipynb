{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitgpmodelcondadf662a052f7548da8f1cfb5439246441",
   "display_name": "Python 3.6.10 64-bit ('GP_Model': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\Users\\kikir\\anaconda3\\envs\\GP_Model\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
    }
   ],
   "source": [
    "import encoding_tools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# ML imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "from scipy import optimize, linalg\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom imports\n",
    "import encoding_tools as encoding\n",
    "import chimera_tools as chimera\n",
    "import GP_tools as GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Folder = Path(r\"Phosphotase_Encode.ipynb\").parent.absolute() / Path(\"Processed Data\")\n",
    "\n",
    "dicts = ['EFI_ID_List', 'metabolite_dict', 'Protein_seq_dict']\n",
    "\n",
    "with open(Processed_Folder / Path('EFI_ID_List.p'), 'rb') as EFI_ID:\n",
    "    EFI_ID_List = pickle.load(EFI_ID)\n",
    "\n",
    "with open(Processed_Folder / Path('metabolite_dict.p'), 'rb') as metabolite:\n",
    "    metabolite_dict = pickle.load(metabolite)\n",
    "\n",
    "with open(Processed_Folder / Path('Protein_seq_dict.p'), 'rb') as Protein_seq:\n",
    "    Protein_seq_dict = pickle.load(Protein_seq)\n",
    "\n",
    "with open(Processed_Folder / Path('Protein_aligned_dict.p'), 'rb') as Protein_aln:\n",
    "    Protein_aligned_dict = pickle.load(Protein_aln)\n",
    "\n",
    "activations = pd.read_csv(Processed_Folder / Path('activations.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to pad protein sequences to the max length of the longest one\n",
    "max_len = len(max(Protein_aligned_dict.values(), key=len))\n",
    "fillchar = '-' # This is whats used in the GP-UCB paper\n",
    "Padded_dict = {}\n",
    "OH_dict = {}\n",
    "for ID in EFI_ID_List:\n",
    "    Padded_dict[ID] = Protein_seq_dict[ID].upper().ljust(max_len, fillchar)\n",
    "    OH_dict[ID] = encoding_tools.one_hot_seq(seq_input=Padded_dict[ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID = np.random.randint(low=0,high=218)\n",
    "len_comp = len(Padded_dict[EFI_ID_List[ID]])\n",
    "len(Padded_dict[EFI_ID_List[0]]) == len(Padded_dict[EFI_ID_List[ID]])\n",
    "print(len_comp)\n",
    "print(ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_train(X, y):\n",
    "    # test the optimization of the hyp-prams\n",
    "    initial_guess = [0.9,0.9]\n",
    "\n",
    "    # take the log of the initial guess for optimiziation \n",
    "    initial_guess_log = np.log(initial_guess)\n",
    "\n",
    "    # optimize to fit model\n",
    "    result = scipy.optimize.minimize(GP.neg_log_marg_likelihood, initial_guess_log, args=(X,y), method='L-BFGS-B')\n",
    "    \n",
    "    print('Full GP regression model')\n",
    "    print('Hyperparameters: ' + str(np.exp(result.x[0])) + ' ' + str(np.exp(result.x[1])))\n",
    "\n",
    "    # next set of hyper prams \n",
    "    final_prams = [np.exp(result.x[0]), np.exp(result.x[1])]\n",
    "    \n",
    "    return final_prams\n",
    "\n",
    "def ML_predict(X, y, X_true_test, y_true_test, log_data, final_prams, property_, num_iter=1, Substrate_ID=0, metabolite_dict=metabolite_dict):\n",
    "    substrate = list(metabolite_dict.values())[Substrate_ID]\n",
    "    \n",
    "    if not os.path.exists('outputs/loop_figs_aln/' + str(substrate) + '/'):\n",
    "        os.makedirs('outputs/loop_figs_aln/' + str(substrate) + '/')\n",
    "\n",
    "    path_outputs = 'outputs/loop_figs_aln/' + str(substrate) + '/'\n",
    "\n",
    "    # next use trained GP model to predict full test set\n",
    "    mu_true_test, var_true_test = GP.predict_GP(X, y, X_true_test, final_prams)\n",
    "\n",
    "    # convert the true test predications and y back to unnormalized data\n",
    "    y_test_real = np.exp(y_true_test*np.std(log_data)  + np.mean(log_data))\n",
    "    mu_test_real = np.exp(mu_true_test*np.std(log_data)  + np.mean(log_data))\n",
    "\n",
    "    if property_ != 'kinetics_off':\n",
    "        \n",
    "        par = np.polyfit(y_test_real, mu_test_real, 1, full=True)\n",
    "        slope=par[0][0]\n",
    "        intercept=par[0][1]\n",
    "        \n",
    "        # coefficient of determination, plot text\n",
    "        variance = np.var(mu_test_real)\n",
    "        residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(y_test_real, mu_test_real)])\n",
    "        Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "        print('GP regression model test set')\n",
    "        print('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        \n",
    "        # plot and measure correlation\n",
    "        plt.figure('True test', figsize=(1.5, 1.5))\n",
    "        plt.plot(y_test_real, mu_test_real, 'o', ms=3, color='k')\n",
    "        \n",
    "        max_x = np.max(y_test_real)\n",
    "        plt.plot([0, max_x], [intercept, slope*max_x+intercept], '-', color='k')\n",
    "        plt.suptitle('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        plt.savefig(path_outputs + str(property_)+'_matern_kernel_'+str(num_iter)+'.png', bbox_inches='tight', transparent=False, dpi=300)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    elif property_ == 'kinetics_off':\n",
    "        \n",
    "        par = np.polyfit(np.log10(y_test_real), np.log10(mu_test_real), 1, full=True)\n",
    "        slope=par[0][0]\n",
    "        intercept=par[0][1]\n",
    "        \n",
    "        # coefficient of determination, plot text\n",
    "        variance = np.var(np.log10(mu_test_real))\n",
    "        residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(np.log10(y_test_real), np.log10(mu_test_real))])\n",
    "        Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "        print('GP regression model test set')\n",
    "        print('R = %0.3f'% np.sqrt(Rsqr))\n",
    "        \n",
    "        # plot and measure correlation\n",
    "        plt.figure('True test', figsize=(1.5, 1.5))\n",
    "        plt.plot(np.log10(y_test_real), np.log10(mu_test_real), 'o',  ms=3, color='k')\n",
    "        \n",
    "        max_x = np.max(y_test_real)\n",
    "        min_x = np.min(y_test_real)\n",
    "        \n",
    "        plt.plot([np.log10(min_x), np.log10(max_x)], [np.log10(slope*min_x+intercept), np.log10(slope*max_x+intercept)], '-', color='k')\n",
    "        \n",
    "        plt.savefig(path_outputs + str(property_)+'_matern_kernel_'+str(num_iter)+'.png', bbox_inches='tight', transparent=True)\n",
    "        # plt.show()\n",
    "\n",
    "    # df_select_test_not_defined\n",
    "    df_select_test = pd.DataFrame(columns=['y','mu','y_real','mu_real'])\n",
    "\n",
    "    # export csv with predicted values\n",
    "    df_select_test['y'] = y_true_test\n",
    "    df_select_test['mu'] = mu_true_test\n",
    "    df_select_test['y_real'] = y_test_real\n",
    "    df_select_test['mu_real'] = mu_test_real\n",
    "\n",
    "    df_select_test.to_csv(path_outputs+ 'matern_kernel_'+str(num_iter)+'_'+str(property_)+'.csv')\n",
    "    return np.sqrt(Rsqr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, log_data, property_):\n",
    "    path_outputs = 'outputs/loop_figs/'\n",
    "\n",
    "    kf = KFold(n_splits=20) # Define the split\n",
    "    kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "    mu_s = []\n",
    "    var_s = []\n",
    "    y_s = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "\n",
    "        log_data_train, log_data_test = log_data[train_index], log_data[test_index]\n",
    "\n",
    "        y_train = (log_data_train - np.mean(log_data_train))/np.std(log_data_train)\n",
    "        y_test = (log_data_test - np.mean(log_data_train))/np.std(log_data_train)\n",
    "\n",
    "        initial_guess = [0.1,10]\n",
    "\n",
    "        # take the log of the initial guess for optimiziation \n",
    "        initial_guess_log = np.log(initial_guess)\n",
    "\n",
    "        # optimize to fit model\n",
    "        result = scipy.optimize.minimize(GP.neg_log_marg_likelihood, initial_guess_log, args=(X_train,y_train), method='L-BFGS-B')#,\n",
    "\n",
    "        # next set of hyper prams \n",
    "        prams_me = [np.exp(result.x[0])**2, np.exp(result.x[1])]\n",
    "\n",
    "        # next used trained GP model to predict on test data\n",
    "        mu, var = GP.predict_GP(X_train, y_train, X_test, prams_me)\n",
    "        \n",
    "        # un normalize\n",
    "        y_test_real = np.exp(y_test*np.std(log_data_train)  + np.mean(log_data_train))\n",
    "        mu_real = np.exp(mu*np.std(log_data_train)  + np.mean(log_data_train))\n",
    "        \n",
    "        mu_s.append(mu)\n",
    "        var_s.append(var)\n",
    "        y_s.append(y_test)\n",
    "\n",
    "    # reformat all\n",
    "    y_s_all = [j for i in y_s for j in i]\n",
    "    mu_s_all = [j for i in mu_s for j in i]\n",
    "\n",
    "    # plot results\n",
    "    plt.figure('My GP test set evaluation', figsize=(1.5, 1.5))\n",
    "    plt.plot(y_s_all, mu_s_all, 'o', ms=3, color='k')\n",
    "\n",
    "\n",
    "    # calculate correlation \n",
    "    measured = y_s_all\n",
    "    predicted = mu_s_all\n",
    "\n",
    "    par = np.polyfit(measured, predicted, 1, full=True)\n",
    "    slope=par[0][0]\n",
    "    intercept=par[0][1]\n",
    "\n",
    "    # calc correlation \n",
    "    variance = np.var(predicted)\n",
    "    residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(measured, predicted)])\n",
    "    Rsqr = np.round(1-residuals/variance, decimals=2)\n",
    "    \n",
    "    print('20-fold corss validation of GP regression model')\n",
    "    print('R = %0.2f'% np.sqrt(Rsqr))\n",
    "\n",
    "    max_x = np.max(y_s_all)\n",
    "    min_x = np.min(y_s_all)\n",
    "    \n",
    "    plt.plot([min_x, max_x], [slope*min_x+intercept, slope*max_x+intercept], '-', color='k')\n",
    "    plt.savefig(path_outputs + str(property_)+'_matern_kernel_CV_fig1.pdf', bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "    return measured, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform CV train\n",
    "# cross_validation(X=X, log_data=log_data, property_='activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_length = [0]*218\n",
    "\n",
    "for i in range(0,len(Protein_seq_dict.values())):\n",
    "    p_length[i] = len(Protein_seq_dict[EFI_ID_List[i]])\n",
    "\n",
    "plt.hist(p_length,bins=600)\n",
    "plt.xlabel('Protein Sequence Length')\n",
    "plt.ylabel('Number of protein sequences')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format(X, y):\n",
    "    # test data only includes gen 10\n",
    "    # df_test_data = df[df.gen == 10]\n",
    "\n",
    "    # training data excludes test data (gen 10)\n",
    "    # df_data = df[df.gen != 10]\n",
    "\n",
    "    # Clean 0 y data which skews results when having to convert infs\n",
    "    X = X[y != 0]\n",
    "    y = y[y != 0]\n",
    "\n",
    "    # Use random split of data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # normalize training data\n",
    "    log_data = np.log(y_train)\n",
    "\n",
    "    '''\n",
    "    # Convert -infs to large negative values\n",
    "    for i in range(0,len(log_data)):\n",
    "        if str(log_data[i]) == '-inf':\n",
    "            log_data[i] = -1000\n",
    "    '''\n",
    "\n",
    "    y_train = (log_data - np.mean(log_data))/np.std(log_data)\n",
    "    # seq = df_select.seq.values\n",
    "\n",
    "    # normalize test data\n",
    "    log_data_test = np.log(y_test)\n",
    "    y_test = (log_data_test - np.mean(log_data))/np.std(log_data)\n",
    "    # seq_test = df_select_test.seq.values\n",
    "\n",
    "    return log_data, X_train, X_test, y_train, y_test \n",
    "    \n",
    "    # seq_test, df_select, df_select_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(Protein_seq_dict,EFI_ID_List,activations,Substrate_ID=0,trim_long=False):\n",
    "    \n",
    "    # Trim length of protein sequences first\n",
    "    if trim_long == True:\n",
    "        Trimmed_dict = {}\n",
    "        New_ID_List = []\n",
    "        for ID in EFI_ID_List:\n",
    "            if len(Protein_seq_dict[ID]) <= 300:\n",
    "                Trimmed_dict[ID] = Protein_seq_dict[ID]\n",
    "                New_ID_List.append(ID)\n",
    "        Protein_seq_dict = Trimmed_dict\n",
    "        EFI_ID_List = New_ID_List\n",
    "    \n",
    "    # Need to pad protein sequences to the max length of the longest one\n",
    "    max_len = len(max(Protein_seq_dict.values(), key=len))\n",
    "    fillchar = '-' # This is whats used in the GP-UCB paper\n",
    "    Padded_dict = {}\n",
    "    OH_dict = {}\n",
    "    for ID in EFI_ID_List:\n",
    "        Padded_dict[ID] = Protein_seq_dict[ID].upper().ljust(max_len, fillchar)\n",
    "        OH_dict[ID] = encoding_tools.one_hot_seq(seq_input=Padded_dict[ID])\n",
    "\n",
    "    # Preparing input training data X to feed into ML Model\n",
    "    input_len = len(OH_dict[EFI_ID_List[0]])*21\n",
    "    num_inputs = len(OH_dict.keys())\n",
    "\n",
    "    X = np.zeros((num_inputs,input_len))\n",
    "    for i in range(0,len(EFI_ID_List)):\n",
    "        ID = EFI_ID_List[i]\n",
    "        X_seq = OH_dict[ID]\n",
    "        X_seq = np.reshape(X_seq,(1,X_seq.shape[0]*21))\n",
    "        X[i,:] = X_seq\n",
    "\n",
    "    # Preapre output training data y to feed into ML Model\n",
    "    dummy = [str(ID) for ID in EFI_ID_List]\n",
    "    y = activations[dummy].values[Substrate_ID,:]\n",
    "\n",
    "    '''\n",
    "    # Now we need to normalize the data\n",
    "    ss1 = preprocessing.StandardScaler()\n",
    "    y = y.reshape(-1,1) # Single feature dataset\n",
    "    y = ss1.fit_transform(y) # fit the SS1 and standardize x_train\n",
    "    y = ss1.transform(y) # transform x_test\n",
    "    '''\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_train_test(Protein_seq_dict, EFI_ID_List, activations, Substrate_ID, num_iter, metabolite_dict=metabolite_dict):\n",
    "    # Need to prep inputs X and Y\n",
    "    X, y = prep_inputs(Protein_seq_dict,EFI_ID_List,activations,Substrate_ID=Substrate_ID,trim_long=True)\n",
    "\n",
    "    # Format data for train and test splits\n",
    "    log_data, X_train, X_test, y_train, y_test = data_format(X, y)\n",
    "\n",
    "    # Train ML Model on training set\n",
    "    final_prams = ML_train(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    R = ML_predict(X=X_train, y=y_train, X_true_test=X_test, y_true_test=y_test, log_data=log_data, final_prams=final_prams, property_='activity', num_iter=num_iter, Substrate_ID=Substrate_ID, metabolite_dict=metabolite_dict)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "metabolite_list = list(metabolite_dict.values())\n",
    "R_Results = pd.DataFrame(columns=metabolite_list)\n",
    "\n",
    "r_low = 0 # Index of starting metabolite in metabolite list (substrates)\n",
    "r_high = 80 # 168 total substrates, do batches of 80\n",
    "num_iters = 20 # 20 trials of split, train, test\n",
    "\n",
    "for j in range(r_low,r_high):\n",
    "    for i in range(0,num_iters):\n",
    "        try:\n",
    "            R = auto_train_test(Protein_seq_dict, EFI_ID_List, activations, Substrate_ID=j, num_iter=i,  metabolite_dict=metabolite_dict)\n",
    "            R_Results.iloc[j,i] = R\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('LinAlgError, negative value in eigenvector')\n",
    "\n",
    "path_outputs = 'outputs/loop_figs/'\n",
    "R_Results.to_csv(path_outputs+ 'R_Values_Metabolites:'+str(r_low)+'_'+str(r_high)+'|'+str(num_iters)+'times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Blank, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: phosphoenolpyruvate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Glycerol-2-phospate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Glycerol-3-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: D-erythronate-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: L-erythronate-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: D-threonate-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Pyrophosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: ATP, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: TTP, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: GTP, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: dATP, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Glycolic Acid O-P, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: Glycerol-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: DL-glyceraldehyde-3-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: dihydroxyacetone phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: meso-erythritol-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: D-threitol-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: D-erythrose-4-phosphate, dtype: object\n0     NaN\n1     NaN\n2     NaN\n3     NaN\n4     NaN\n5     NaN\n6     NaN\n7     NaN\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n14    NaN\n15    NaN\n16    NaN\n17    NaN\n18    NaN\n19    NaN\nName: imidodiphosphate (PNP), dtype: object\n"
    }
   ],
   "source": [
    "R_Results = pd.DataFrame(columns=list(metabolite_dict.values()), index=list(range(0,20)))\n",
    "for i in range(0,20):\n",
    "    print(R_Results.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "168"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Do 0 - 80 metabolites\n",
    "# Next 81 - 168\n",
    "len(metabolite_dict)"
   ]
  }
 ]
}